{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpassignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytzRVhls_1I4",
        "colab_type": "text"
      },
      "source": [
        "# **Multi-class Classification using a multilayered 1D convolution network in PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRBaiiJoB0J3",
        "colab_type": "text"
      },
      "source": [
        "**Prerequisites**\n",
        "> First, connect to a GPU runtime (Runtime > Change runtime type > Hardware accelerator > GPU > Save, then connect)\n",
        "\n",
        "> Next, let's import the following libraries to work with our dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ms9MOZFBNB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Get the train/test split package from sklearn for preparing our dataset to\n",
        "# train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import the numpy library to work with and manipulate the data\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6OIDfAzEClQ",
        "colab_type": "text"
      },
      "source": [
        "**Processing our dataset**\n",
        "> First, let's read and clean the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HcwsclgmuU52",
        "colab": {}
      },
      "source": [
        "# import data from google drive\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ah48StzR66m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresofdata=data[['PhraseId', 'SentenceId','Phrase','Sentiment']] # please note I am not using sentiment. It is just for treating it as data.\n",
        "labelsofdata=data[['Sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IviWnp9WSKaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # split data in the beginning as per the instruction using train_test_split in 70:30 ration and random state 2003.\n",
        "trainX, trainy,testX,testy=train_test_split(featuresofdata, labelsofdata, test_size=0.3,random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iziaKiOsMyva",
        "colab_type": "code",
        "outputId": "39683be2-f0a9-495d-e50c-bee96d31c1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Check the head of the dataframe\n",
        "data.head(10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "5         6  ...          2\n",
              "6         7  ...          2\n",
              "7         8  ...          2\n",
              "8         9  ...          2\n",
              "9        10  ...          2\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Asm1bLvzzK0",
        "colab_type": "code",
        "outputId": "bd5398ab-5cdf-43ef-c403-01f4891fa91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check the shape of df\n",
        "data.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156060, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFqJlDFiNiHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get number of unique sentences\n",
        "numSentences = data['SentenceId'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9i_Fg2qzzB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract full sentences only from the dataset\n",
        "fullSentences = []\n",
        "curSentence = 0\n",
        "for i in range(data.shape[0]):\n",
        "  if data['SentenceId'][i]> curSentence:\n",
        "    fullSentences.append((data['Phrase'][i], data['Sentiment'][i]))\n",
        "    curSentence = curSentence +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrA-E9LH5M_f",
        "colab_type": "code",
        "outputId": "80831864-5457-466a-fa61-9d1e4c125c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(fullSentences)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH7RwKy0zyx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# put data into a df\n",
        "fullSentDf = pd.DataFrame(fullSentences,\n",
        "                                columns=['Phrase', 'Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswLOk8gO5am",
        "colab_type": "code",
        "outputId": "988892bf-46f5-4753-b043-a32f6874c96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Check class imbalance in tokenized sentences\n",
        "data['Sentiment'].value_counts()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhBKSBtO46D",
        "colab_type": "code",
        "outputId": "2caeef41-99c0-4080-dfea-be77ecdf044b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Check class imbalance in full sentences\n",
        "fullSentDf['Sentiment'].value_counts()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2325\n",
              "1    2203\n",
              "2    1659\n",
              "4    1282\n",
              "0    1075\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwInoiU77Aoa",
        "colab_type": "code",
        "outputId": "c9c17fe4-d6fb-443c-c0ba-a5da19d5765c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78shXqXGPwuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = []\n",
        "#convert data into format for the previous labs\n",
        "\n",
        "#use full dataset\n",
        "#for i in range(data.shape[0]):\n",
        "#  tmpWords = word_tokenize(data['Phrase'][i])\n",
        "#  documents.append((tmpWords, data['Sentiment'][i]))\n",
        "\n",
        "# Use only complete sentences\n",
        "for i in range(fullSentDf.shape[0]):\n",
        "  tmpWords = word_tokenize(fullSentDf['Phrase'][i])\n",
        "  documents.append((tmpWords, fullSentDf['Sentiment'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfDx542WSfjJ",
        "colab_type": "code",
        "outputId": "3a813615-2a1a-4b9d-b0b6-57faa8d2cfec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "random.seed(9001)\n",
        "random.shuffle(documents)\n",
        "print(documents[1][0])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['For', 'its', '100', 'minutes', 'running', 'time', ',', 'you', \"'ll\", 'wait', 'in', 'vain', 'for', 'a', 'movie', 'to', 'happen', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ngNm5nGS5x6",
        "colab_type": "code",
        "outputId": "02bf0912-6262-4592-e99f-5595ecbf99d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(documents)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTgjvUD17Jwp",
        "colab_type": "code",
        "outputId": "3760f7be-2308-4cda-84cf-20d3c88f482e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations=\"?:!.,;'\\\"-()\"\n",
        "\n",
        "#parameters to adjust to see the impact on outcome\n",
        "remove_stopwords = True\n",
        "useStemming = True\n",
        "useLemma = False\n",
        "removePuncs = True\n",
        "\n",
        "for l in range(len(documents)):\n",
        "  label = documents[l][1]\n",
        "  tmpReview = []\n",
        "  for w in documents[l][0]:\n",
        "    newWord = w\n",
        "    if remove_stopwords and (w in stopwords_en):\n",
        "      continue\n",
        "    if removePuncs and (w in punctuations):\n",
        "      continue\n",
        "    if useStemming:\n",
        "      #newWord = porter.stem(newWord)\n",
        "      newWord = lancaster.stem(newWord)\n",
        "    if useLemma:\n",
        "      newWord = wordnet_lemmatizer.lemmatize(newWord)\n",
        "    tmpReview.append(newWord)\n",
        "  documents[l] = (' '.join(tmpReview), label)\n",
        "print(documents[2])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ian holm conqu frant earthy napoleon', 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglN1mwuULw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = pd.DataFrame(documents,\n",
        "                                columns=['text', 'sentiment'])\n",
        "# Splits the dataset so 70% is used for training and 30% for testing\n",
        "x_train_raw, x_test_raw, y_train_raw, y_test_raw = train_test_split(all_data['text'], all_data['sentiment'], test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUe044MwUgcE",
        "colab_type": "code",
        "outputId": "232d057d-c234-474b-9c91-0f4bb72bfe5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(x_train_raw)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5980"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcJhv7kU8Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Transform each text into a vector of word counts\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",\n",
        "                             ngram_range=(1, 2))\n",
        "#vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "#                             ngram_range=(1, 2))\n",
        "#X = vectorizer.fit_transform(all_data[\"text\"])\n",
        "#Y = all_data['sentiment']\n",
        "x_train = vectorizer.fit_transform(x_train_raw)\n",
        "y_train = y_train_raw\n",
        "x_test = vectorizer.transform(x_test_raw)\n",
        "y_test = y_test_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqLmDkY97BDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the datasets to numpy arrays to work with our PyTorch model\n",
        "x_train_np = x_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "# Convert the testing data\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk6lX0pv7BIp",
        "colab_type": "code",
        "outputId": "05e396b5-fb92-41df-bf2d-e1dc22400600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_np.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5980, 50491)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_JhOF_6FlQA",
        "colab_type": "text"
      },
      "source": [
        "**Creating the network**\n",
        "> First, let's import the following libraries to build our network with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfaTBPJSFsIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the pytorch library\n",
        "import torch\n",
        "torch.manual_seed(2003)\n",
        "\n",
        "# Import the 1D convolution layer\n",
        "# Since we’re inputting a 1-dimensional row of data, we can’t use 2D or 3D\n",
        "from torch.nn import Conv1d\n",
        "\n",
        "# Import the max pooling layer\n",
        "from torch.nn import MaxPool1d\n",
        "\n",
        "# Import the flatten layer\n",
        "from torch.nn import Flatten\n",
        "\n",
        "# Import the linear layer\n",
        "from torch.nn import Linear\n",
        "\n",
        "# Import the ReLU & Softmax activation function\n",
        "from torch.nn.functional import relu, softmax, sigmoid\n",
        "\n",
        "# Import the DataLoader and TensorDataset libraries from PyTorch\n",
        "# to work with our datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rokjpDxcGBtp",
        "colab_type": "text"
      },
      "source": [
        "> Next, let's define our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ_R-FtYGW7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our class MUST be a subclass of torch.nn.Module\n",
        "class CnnClassifier(torch.nn.Module):\n",
        "  # Define the initialization method\n",
        "  def __init__(self, batch_size, inputs, outputs):\n",
        "\n",
        "    # Initialize the superclass and store the parameters\n",
        "    super(CnnClassifier, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "\n",
        "    # Define the input layer\n",
        "    # (input channels, output channels, kernel size)\n",
        "    self.input_layer = Conv1d(inputs, batch_size, 1)\n",
        "\n",
        "    # Define a max pooling layer\n",
        "    # (kernel size)\n",
        "    self.max_pooling_layer = MaxPool1d(1)\n",
        "\n",
        "    # Define another convolution layer\n",
        "    self.conv_layer = Conv1d(batch_size, 128, 1)\n",
        "\n",
        "    # Define a flatten layer\n",
        "    self.flatten_layer = Flatten()\n",
        "\n",
        "    # Define a linear layer\n",
        "    # (inputs, outputs)\n",
        "    self.linear_layer = Linear(128, 64)\n",
        "\n",
        "    # Finally, define the output layer\n",
        "    self.output_layer = Linear(64, outputs)\n",
        "\n",
        "  # Define a method to feed inputs through the model\n",
        "  def feed(self, input):\n",
        "    # Reshape the entry so it can be fed to the input layer\n",
        "    # Although we’re using 1D convolution, it still expects a 3D array to \n",
        "    # process in a 1D fashion\n",
        "    input = input.reshape((self.batch_size, self.inputs, 1))\n",
        "\n",
        "    # Get the output of the first layer and run it through the\n",
        "    # the ReLU activation function\n",
        "    output = relu(self.input_layer(input))\n",
        "\n",
        "    # Get the output of the max pooling layer\n",
        "    output = self.max_pooling_layer(output)\n",
        "\n",
        "    # Get the output of the second convolution layer and run it\n",
        "    # through the ReLU activation function\n",
        "    output = relu(self.conv_layer(output))\n",
        "\n",
        "    # Get the output of the flatten layer\n",
        "    output = self.flatten_layer(output)\n",
        "\n",
        "    # Get the output of the linear layer and run it through the\n",
        "    # ReLU activation function\n",
        "    output = self.linear_layer(output)\n",
        "\n",
        "    # Finally, get the output of the output layer and return it\n",
        "    output = self.output_layer(output)\n",
        "    \n",
        "    #We use softmax for multi-class classification\n",
        "    output = softmax(output)\n",
        "   \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx_eOdsuPKX5",
        "colab_type": "text"
      },
      "source": [
        "**Training the model**\n",
        "> First, let's import the optimizer and performance measure we'll be using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NpIoMgVPUtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the SGD (stochastic gradient descent) package from pytorch for\n",
        "# our optimizer\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "# Import the L1Loss (mean absolute error loss) package from pytorch for\n",
        "# our performance measure\n",
        "from torch.nn import L1Loss, CrossEntropyLoss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amFk7CgEPVrh",
        "colab_type": "text"
      },
      "source": [
        "> Next, let's define our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpiTZqhWPo4Z",
        "colab_type": "code",
        "outputId": "6e8a15c1-a38b-4357-9d48-d33fb3641ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Define the batch size we'd like to use\n",
        "batch_size = 256\n",
        "\n",
        "# (batch size, X columns, Y columns)\n",
        "model = CnnClassifier(batch_size, x_train.shape[1], 5)\n",
        "\n",
        "# Set the model to use the GPU for processing\n",
        "model.cuda()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CnnClassifier(\n",
              "  (input_layer): Conv1d(50491, 256, kernel_size=(1,), stride=(1,))\n",
              "  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
              "  (flatten_layer): Flatten()\n",
              "  (linear_layer): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkDZehDziBbj",
        "colab_type": "text"
      },
      "source": [
        "> Next, let's create a method for running the batches of data through our model (this is using code from the first lab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79XIb_jkygpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return (correct.sum() / torch.FloatTensor([y.shape[0]])) / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU5YwKGxzFpN",
        "colab_type": "code",
        "outputId": "8bc5d3c8-4298-488b-b28f-2b1fe46bd06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "indicies=range(5)\n",
        "encoder=LabelBinarizer()\n",
        "labels=encoder.fit_transform(indicies)\n",
        "print(labels[1])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp70eeda_POS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This method will return the average L1 loss and R^2 score \n",
        "# of the passed model on the passed DataLoader\n",
        "def model_loss(model, dataset, train = False, optimizer = None):\n",
        "  # Cycle through the batches and get the average L1 loss\n",
        "  performance = L1Loss()\n",
        "  criterion = CrossEntropyLoss()\n",
        "  avg_loss = 0\n",
        "  avg_accu = 0\n",
        "  count = 0\n",
        "  \n",
        "  for input, output in iter(dataset):\n",
        "    # Get the model's predictions for the training dataset\n",
        "    predictions = model.feed(input)\n",
        "    \n",
        "    # Get the model's loss\n",
        "    loss = performance(predictions, output)\n",
        "    \n",
        "    tmp_accu = categorical_accuracy(predictions, output)\n",
        "    \n",
        "    if(train):\n",
        "      # Clear any errors so they don't cummulate\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Compute the gradients for our optimizer\n",
        "      loss.backward()\n",
        "\n",
        "      # Use the optimizer to update the model's parameters based on the\n",
        "      # gradients\n",
        "      optimizer.step()\n",
        "\n",
        "    # Store the loss and update the counter\n",
        "    avg_loss += loss.item()\n",
        "\n",
        "    # Accumulate performance metrices\n",
        "    avg_accu += tmp_accu.item()\n",
        "    count += 1\n",
        "    \n",
        "  return avg_loss / count, avg_accu / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tvPm_BP4Gb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Finally, let's train the model (this is using code from the first lab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITvKKOe8P9FZ",
        "colab_type": "code",
        "outputId": "cbb982d1-de76-405a-be82-e871a2604e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "def performtraining():\n",
        "  # Define the number of epochs to train for\n",
        "  epochs = 10\n",
        "\n",
        "  # Define the performance measure and optimizer\n",
        "  optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "  #optimizer = Adam(model.parameters())\n",
        "\n",
        "  # Convert the training set into torch variables for our model using the GPU\n",
        "  # as floats. The reshape is to remove a warning pytorch outputs otherwise.\n",
        "  inputs = torch.from_numpy(x_train_np).cuda().float()\n",
        "  outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "  # Create a DataLoader instance to work with our batches\n",
        "  tensor = TensorDataset(inputs, outputs)\n",
        "  loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "  # Start the training loop\n",
        "  for epoch in range(epochs):\n",
        "    # Cycle through the batches and get the average loss\n",
        "    avg_loss, avg_accu = model_loss(model, loader, train=True, optimizer=optimizer)\n",
        "    print(\"Epoch \" + str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss))\n",
        "    print(\"Accuracy = \" + str(avg_accu))\n",
        "    \n",
        "performtraining()  "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1:\n",
            "\tLoss = 1.9084579063498455\n",
            "Accuracy = 0.262398097826087\n",
            "Epoch 2:\n",
            "\tLoss = 1.9099524591280066\n",
            "Accuracy = 0.26154891304347827\n",
            "Epoch 3:\n",
            "\tLoss = 1.9078804461852363\n",
            "Accuracy = 0.26154891304347827\n",
            "Epoch 4:\n",
            "\tLoss = 1.9080842474232549\n",
            "Accuracy = 0.26290760869565216\n",
            "Epoch 5:\n",
            "\tLoss = 1.906555730363597\n",
            "Accuracy = 0.2630774456521739\n",
            "Epoch 6:\n",
            "\tLoss = 1.9077785637067712\n",
            "Accuracy = 0.26222826086956524\n",
            "Epoch 7:\n",
            "\tLoss = 1.908118232436802\n",
            "Accuracy = 0.2630774456521739\n",
            "Epoch 8:\n",
            "\tLoss = 1.9087975854459016\n",
            "Accuracy = 0.26154891304347827\n",
            "Epoch 9:\n",
            "\tLoss = 1.9120584519013115\n",
            "Accuracy = 0.26205842391304346\n",
            "Epoch 10:\n",
            "\tLoss = 1.905298932738926\n",
            "Accuracy = 0.26222826086956524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiF8QFwDEO7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e6e7e229-c37f-4a1f-9100-cfc112144ac4"
      },
      "source": [
        "#saving model to disk\n",
        "torch.save(model, '1099371_1dconv_reg.pt')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CnnClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WTPkJYQE_lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "98672fb5-3453-44da-d09e-fd65c476ddc0"
      },
      "source": [
        "# loading model from disk\n",
        "model = torch.load('1099371_1dconv_reg.pt')\n",
        "model.eval()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CnnClassifier(\n",
              "  (input_layer): Conv1d(50491, 256, kernel_size=(1,), stride=(1,))\n",
              "  (max_pooling_layer): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
              "  (flatten_layer): Flatten()\n",
              "  (linear_layer): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ugbkGFhMXL",
        "colab_type": "text"
      },
      "source": [
        "> Lastly, we will test the model to see how it performs on the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NolMttQ-HA5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "895d5b7f-b525-4c3c-b3a9-17dab31dda2e"
      },
      "source": [
        "def performtesting():\n",
        "  # Convert the testing set into torch variables for our model using the GPU as floats\n",
        "  inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "  outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "  # Create a DataLoader instance to work with our batches\n",
        "  tensor = TensorDataset(inputs, outputs)\n",
        "  loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "  avg_loss, avg_accu = model_loss(model, loader)\n",
        "  \n",
        "  print(\"The model's L1 loss is: \" + str(avg_loss))\n",
        "  print(\"The model's Accuracy is: \" + str(avg_accu))\n",
        "\n",
        "performtesting()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The model's L1 loss is: 1.9233594059944152\n",
            "The model's Accuracy is: 0.248046875\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}